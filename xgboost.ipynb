{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "#https://xgboost.readthedocs.io/en/latest/python/index.html\n",
    "# https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/train.py#L35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m__pycache__\u001b[m\u001b[m                train-images-idx3-ubyte.gz\r\n",
      "load_mnist.py              train-labels-idx1-ubyte.gz\r\n",
      "t10k-images-idx3-ubyte.gz  xgboost.ipynb\r\n",
      "t10k-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_mnist import load_mnist\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "train_data, train_label = load_mnist(\"\",\"train\")\n",
    "test_data,test_label = load_mnist(\"\",\"t10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler=preprocessing.MinMaxScaler()  \n",
    "min_max=min_max_scaler.fit(train_data)\n",
    "\n",
    "train_data = min_max.transform(train_data)\n",
    "test_data = min_max.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'multi:softmax', 'num_class': 10, 'silent': 1, 'max_depth': 8}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# num_round = [2,10,50,150]\n",
    "# param['max_depth'] = [2, 4, 6, 8]\n",
    "\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "# param['eta'] = 0.3\n",
    "param['num_class'] = 10\n",
    "\n",
    "param['silent'] = 1\n",
    "# param['nthread'] = 6\n",
    "\n",
    "num_round = 2\n",
    "param['max_depth'] = 8\n",
    "print(param)\n",
    "print(num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.077625\teval-merror:0.213846\n",
      "[1]\ttrain-merror:0.051875\teval-merror:0.1905\n",
      "Test accuracy using softmax = 0.805\n"
     ]
    }
   ],
   "source": [
    "# bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "\n",
    "# bst = xgb.train(param, dtrain, num_round)\n",
    "# for i in range(1):\n",
    "\n",
    "\n",
    "arr = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "dtrain = xgb.DMatrix(train_data[arr[:8000]], label=train_label[arr[:8000]])\n",
    "dval = xgb.DMatrix(train_data[arr[8000:]], label=train_label[arr[8000:]])\n",
    "\n",
    "dtest = xgb.DMatrix(test_data, label=test_label)\n",
    "evallist = [(dtrain, 'train'),(dval, 'eval')]\n",
    "\n",
    "\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)\n",
    "pred = bst.predict(dtest)\n",
    "acc_rate = np.sum(pred == test_label) / test_label.shape[0]\n",
    "print('Test accuracy using softmax = {}'.format(acc_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/python\n",
    "# import numpy as np\n",
    "# import xgboost as xgb\n",
    "\n",
    "# ### load data in do training\n",
    "# param = {}\n",
    "# # use softmax multi-class classification\n",
    "# param['objective'] = 'multi:softmax'\n",
    "# # scale weight of positive examples\n",
    "# param['eta'] = 0.1\n",
    "# param['max_depth'] = 10\n",
    "# param['silent'] = 1\n",
    "# param['nthread'] = 6\n",
    "# param['num_class'] = 10\n",
    "# num_round = 2\n",
    "\n",
    "# xgb.cv(param, dtrain, num_round, nfold=5, metrics={'auc'}, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
